{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYPD Fare Evasion Enforcement's Disparate Impact\n",
    "Leena Kahlon, Dylan Fosgett, Woo Hyuk Chang and Rebecca Dorn\n",
    "University of California, Santa Cruz, Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/media/EJAwu4gWsAoPKwL?format=jpg&name=medium\">\n",
    "\n",
    "*Image from Twitter's @DecolonizeThisPlace*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***\n",
    "Imagine your loan application has just been rejected by your bank. You ask them why, and they respond that the neighborhood you live in scored poorly for likelihood to pay the loan back. For some reason this bank has the algorithm they use online. You look into it a bit more, and you realize that though the bank does not use race as a feature, those who live in predominantly African American neighborhoods are almost three times as likely to be denied a loan. Disparate impact refers to \"when policies, practices, rules or other systems that appear to be neutral result in a disproportionate impact on a protected group\". No one in the bank sat down and say \"hey, I'm going to create a fairly racist predictor!\". But, the rules that the bank goes by dispraportionately impact African Americans. Situations like these are all too common in the United States, and they must be addressed.\n",
    "\n",
    "On October 28, 2019, a video of around ten police officers tackling and pulling guns on a black teen for evading the \\$2.75 subway fare in New York City went viral. Communities were enraged to see this fierceley violent attack on a teen of color over such a small fare. A few months before, the MTA calculated that they were losing millions annually due to those who evade paying the transit fare. The MTA decided to deploy 500 new officers to watch for turnstile hoppers, in hopes of losing less money. While in theory this plan makes sense, we can't help but wonder. What kind of disparate impact against communities of color is happening? What about against lower income communities?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Installing Necessary Libraries\n",
    "```pip install matplotlib\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install geopandas\n",
    "pip install beautifulsoup4\n",
    "pip install seaborn\n",
    "pip install twitter\n",
    "pip install nltk```\n",
    "\n",
    "Cooresponding files can be found on git: https://github.com/rebdorn/fare_evasion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "***\n",
    "We organize our project into distinct portions\n",
    "1. Web Scraping\n",
    "2. Data Wrangling\n",
    "3. Data Visualization\n",
    "\n",
    "Our goal is to create an easy-to-use visualization to answer the following research questions:\n",
    "1. How has fare evasion enforcement in New York City changed in recent years?\n",
    "2. What subway stations have an outstanding number of recent police sightings? What are the demographics around these stations?\n",
    "3. What are the fare evasione enforcement trends around stations with outstanding numbers of recent police sightings?\n",
    "4. What is the overlap between stations with more police sightings and stations with more fare evasion enforcement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "***\n",
    "To get recent police sightings, we scrape twitter account @unfarenyc. This is an account where people send information about police sightings at subway stations, and they post the number of cops and which subway station. We scrape the twitter account these sightings using nltk and bear's python twitter wrapper. Note that to run this code on your account, you must posess a secret key given by the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import statements to make our code run\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import twitter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions that will be useful in making sense of the data\n",
    "def timeline_to_df(statuses_list,column_names):\n",
    "    tweets = []\n",
    "    for s in statuses_list:\n",
    "        tweets.append([s.id,s.user.screen_name,s.created_at,s.text,s.retweet_count])\n",
    "    return pd.DataFrame(tweets,columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order: scrape (Tweeter), NLTK, showing why we chose some stations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = '0ift2jAgEYQrWs8OP1f9TjRQ1'\n",
    "CONSUMER_SECRET = 'qq9jNTaS3cG9wVc2Hxij3M0GuhAy72wNzFnBc9y1ejp2xh5dg4'\n",
    "ACCESS_TOKEN_KEY = '144353594-pUtB5UxLeZHwZxJH4vPtFQMXAvgyQKTyvDanafog'\n",
    "ACCESS_TOKEN_SECRET = 'TvGH6dRf2J0PTKgBiSwWJ022j25PNBBaYUZQYWML1bDok'\n",
    "\n",
    "# connect to twitter API with our given user credentials\n",
    "api = twitter.Api(consumer_key = CONSUMER_KEY,\n",
    "                      consumer_secret = CONSUMER_SECRET,\n",
    "                      access_token_key = ACCESS_TOKEN_KEY,\n",
    "                      access_token_secret = ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# Create a dataframe to get all @unfarenyc tweets.\n",
    "# Note that the maximum number of tweets we can get at once is 200\n",
    "statuses_list = api.GetUserTimeline(count=200,screen_name='unfarenyc') # Load @unfarenyc's timeline\n",
    "\n",
    "# To get earlier tweets we continuously find the minimum and get the 200 previous\n",
    "earliest_tweet = min(statuses_list, key=lambda x: x.id).id # Find the last tweet we were allowed to get\n",
    "while True: # While there are more tweets to get\n",
    "    nexttweets = api.GetUserTimeline(screen_name='unfarenyc', max_id=earliest_tweet, count=200)\n",
    "    new_earliest = min(nexttweets, key=lambda x: x.id).id\n",
    "    if not nexttweets or new_earliest == earliest_tweet:\n",
    "        break\n",
    "    else:\n",
    "        earliest_tweet = new_earliest\n",
    "        statuses_list += nexttweets\n",
    "\n",
    "tweetsdf = timeline_to_df(statuses_list,['ID','user','created_at','text','retweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our protocol for getting unigrams into a usable form\n",
    "def process_unigram(unigram):\n",
    "    if unigram[0].isdigit() & unigram.endswith('st'): # Are we of form \"1\"st?\n",
    "        unigram = unigram[:-2] # Shorten unigram to only that street number\n",
    "    elif unigram[0].isdigit() & unigram.endswith('nd'): # Are we of form \"2\"nd?\n",
    "        unigram = unigram[:-2] # Shorten unigram to just that number\n",
    "    elif unigram[0].isdigit() and unigram.endswith('th'): # Are we of form \"10\"th?\n",
    "        unigram = unigram[:-2] # Shorten unigram to only that number\n",
    "    elif unigram == 'park': # If this unigram reads 'park'\n",
    "        unigram = 'pk' # Shorten it to match our station names\n",
    "    elif unigram == 'parkway': # If this unigram reads \"parkway\"\n",
    "        unigram = 'pkwy' # Shorten it to match our station names\n",
    "    elif unigram == 'heights': # If this unigram reads \"heights\"\n",
    "        unigram = 'hts' # Shorted it to match our station names\n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "***\n",
    "We further split our data wrangling into three parts:\n",
    "1. Wrangling at the Community District level\n",
    "2. Wrangling at the Transit District level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling at the Community District level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and descriptions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling at the Transit District level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More code and descriptions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "***\n",
    "We visualize data and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More of code and descriptions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "https://nyc.streetsblog.org/2019/11/14/mta-will-spend-249m-on-new-cops-to-save-200m-on-fare-evasion/ , defining disparate impact\n",
    "*These are really really not finished*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
